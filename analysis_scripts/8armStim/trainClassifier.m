function [trainedClassifier, validationAccuracy] = trainClassifier(trainingData)
% [trainedClassifier, validationAccuracy] = trainClassifier(trainingData)
% returns a trained classifier and its accuracy. This code recreates the
% classification model trained in Classification Learner app. Use the
% generated code to automate training the same model with new data, or to
% learn how to programmatically train models.
%
%  Input:
%      trainingData: a matrix with the same number of rows and data type as
%       imported into the app.
%
%  Output:
%      trainedClassifier: a struct containing the trained classifier. The
%       struct contains various fields with information about the trained
%       classifier.
%
%      trainedClassifier.predictFcn: a function to make predictions on new
%       data.
%
%      validationAccuracy: a double containing the accuracy in percent. In
%       the app, the History list displays this overall accuracy score for
%       each model.
%
% Use the code to train the model with new data. To retrain your
% classifier, call the function from the command line with your original
% data or new data as the input argument trainingData.
%
% For example, to retrain a classifier trained with the original data set
% T, enter:
%   [trainedClassifier, validationAccuracy] = trainClassifier(T)
%
% To make predictions with the returned 'trainedClassifier' on new data T2,
% use
%   yfit = trainedClassifier.predictFcn(T2)
%
% T2 must be a matrix containing only the predictor rows used for training.
% For details, enter:
%   trainedClassifier.HowToPredict

% Auto-generated by MATLAB on 12-Jun-2018 23:57:12


% Extract predictors and response
% This code processes the data into the right shape for training the
% model.
% Convert input to table
inputTable = array2table(trainingData', 'VariableNames', {'row_1', 'row_2', 'row_3', 'row_4', 'row_5', 'row_6', 'row_7', 'row_8', 'row_9', 'row_10', 'row_11', 'row_12', 'row_13', 'row_14', 'row_15', 'row_16', 'row_17', 'row_18', 'row_19', 'row_20', 'row_21', 'row_22', 'row_23', 'row_24', 'row_25', 'row_26', 'row_27', 'row_28', 'row_29', 'row_30', 'row_31', 'row_32', 'row_33', 'row_34', 'row_35', 'row_36', 'row_37', 'row_38', 'row_39', 'row_40', 'row_41', 'row_42', 'row_43', 'row_44', 'row_45', 'row_46', 'row_47', 'row_48', 'row_49', 'row_50', 'row_51', 'row_52', 'row_53', 'row_54', 'row_55', 'row_56', 'row_57', 'row_58', 'row_59', 'row_60', 'row_61', 'row_62', 'row_63', 'row_64', 'row_65', 'row_66', 'row_67', 'row_68', 'row_69', 'row_70', 'row_71', 'row_72', 'row_73', 'row_74', 'row_75', 'row_76', 'row_77', 'row_78', 'row_79', 'row_80', 'row_81', 'row_82', 'row_83', 'row_84', 'row_85', 'row_86', 'row_87', 'row_88', 'row_89', 'row_90', 'row_91', 'row_92', 'row_93', 'row_94', 'row_95', 'row_96', 'row_97', 'row_98', 'row_99', 'row_100', 'row_101', 'row_102', 'row_103', 'row_104', 'row_105', 'row_106', 'row_107', 'row_108', 'row_109', 'row_110', 'row_111', 'row_112', 'row_113', 'row_114', 'row_115', 'row_116', 'row_117', 'row_118', 'row_119', 'row_120', 'row_121', 'row_122', 'row_123', 'row_124', 'row_125', 'row_126', 'row_127', 'row_128', 'row_129', 'row_130', 'row_131', 'row_132', 'row_133', 'row_134', 'row_135', 'row_136', 'row_137', 'row_138', 'row_139', 'row_140', 'row_141', 'row_142', 'row_143', 'row_144', 'row_145', 'row_146', 'row_147', 'row_148', 'row_149', 'row_150', 'row_151', 'row_152', 'row_153', 'row_154', 'row_155', 'row_156', 'row_157', 'row_158', 'row_159', 'row_160', 'row_161', 'row_162', 'row_163', 'row_164', 'row_165', 'row_166', 'row_167', 'row_168', 'row_169', 'row_170', 'row_171', 'row_172', 'row_173', 'row_174', 'row_175', 'row_176', 'row_177', 'row_178', 'row_179', 'row_180', 'row_181', 'row_182', 'row_183', 'row_184', 'row_185', 'row_186', 'row_187', 'row_188', 'row_189', 'row_190', 'row_191', 'row_192', 'row_193', 'row_194', 'row_195', 'row_196', 'row_197', 'row_198', 'row_199', 'row_200', 'row_201', 'row_202', 'row_203', 'row_204', 'row_205', 'row_206', 'row_207', 'row_208', 'row_209', 'row_210', 'row_211', 'row_212', 'row_213', 'row_214', 'row_215', 'row_216', 'row_217', 'row_218', 'row_219', 'row_220', 'row_221', 'row_222', 'row_223', 'row_224', 'row_225', 'row_226', 'row_227', 'row_228', 'row_229', 'row_230', 'row_231', 'row_232', 'row_233', 'row_234', 'row_235', 'row_236', 'row_237', 'row_238', 'row_239', 'row_240', 'row_241', 'row_242', 'row_243', 'row_244', 'row_245', 'row_246', 'row_247', 'row_248', 'row_249', 'row_250', 'row_251', 'row_252', 'row_253', 'row_254', 'row_255', 'row_256', 'row_257', 'row_258', 'row_259', 'row_260', 'row_261', 'row_262', 'row_263', 'row_264', 'row_265', 'row_266', 'row_267', 'row_268', 'row_269', 'row_270', 'row_271', 'row_272', 'row_273', 'row_274', 'row_275', 'row_276', 'row_277', 'row_278', 'row_279', 'row_280', 'row_281', 'row_282', 'row_283', 'row_284', 'row_285', 'row_286', 'row_287', 'row_288', 'row_289', 'row_290', 'row_291', 'row_292', 'row_293', 'row_294', 'row_295', 'row_296', 'row_297', 'row_298', 'row_299', 'row_300', 'row_301', 'row_302', 'row_303', 'row_304', 'row_305', 'row_306', 'row_307', 'row_308', 'row_309', 'row_310', 'row_311', 'row_312', 'row_313', 'row_314', 'row_315', 'row_316', 'row_317', 'row_318', 'row_319', 'row_320', 'row_321', 'row_322', 'row_323', 'row_324', 'row_325', 'row_326', 'row_327', 'row_328', 'row_329', 'row_330', 'row_331', 'row_332', 'row_333', 'row_334', 'row_335', 'row_336', 'row_337', 'row_338', 'row_339', 'row_340', 'row_341', 'row_342', 'row_343', 'row_344', 'row_345', 'row_346', 'row_347', 'row_348', 'row_349', 'row_350', 'row_351', 'row_352', 'row_353', 'row_354', 'row_355', 'row_356', 'row_357', 'row_358', 'row_359', 'row_360', 'row_361', 'row_362', 'row_363', 'row_364', 'row_365', 'row_366', 'row_367', 'row_368', 'row_369', 'row_370', 'row_371', 'row_372', 'row_373', 'row_374', 'row_375', 'row_376', 'row_377', 'row_378', 'row_379', 'row_380', 'row_381', 'row_382', 'row_383', 'row_384', 'row_385', 'row_386', 'row_387', 'row_388', 'row_389', 'row_390', 'row_391', 'row_392', 'row_393', 'row_394', 'row_395', 'row_396', 'row_397', 'row_398', 'row_399', 'row_400', 'row_401', 'row_402', 'row_403', 'row_404', 'row_405', 'row_406', 'row_407', 'row_408', 'row_409', 'row_410', 'row_411', 'row_412', 'row_413', 'row_414', 'row_415', 'row_416', 'row_417', 'row_418', 'row_419', 'row_420', 'row_421', 'row_422', 'row_423', 'row_424', 'row_425', 'row_426', 'row_427', 'row_428', 'row_429', 'row_430', 'row_431', 'row_432', 'row_433', 'row_434', 'row_435', 'row_436', 'row_437', 'row_438', 'row_439', 'row_440', 'row_441', 'row_442', 'row_443', 'row_444', 'row_445', 'row_446', 'row_447', 'row_448', 'row_449', 'row_450', 'row_451', 'row_452', 'row_453', 'row_454', 'row_455', 'row_456', 'row_457', 'row_458', 'row_459', 'row_460', 'row_461', 'row_462'});

predictorNames = {'row_1', 'row_2', 'row_3', 'row_4', 'row_5', 'row_6', 'row_7', 'row_8', 'row_9', 'row_10', 'row_11', 'row_12', 'row_13', 'row_14', 'row_15', 'row_16', 'row_17', 'row_18', 'row_19', 'row_20', 'row_21', 'row_22', 'row_23', 'row_24', 'row_25', 'row_26', 'row_27', 'row_28', 'row_29', 'row_30', 'row_31', 'row_32', 'row_33', 'row_34', 'row_35', 'row_36', 'row_37', 'row_38', 'row_39', 'row_40', 'row_41', 'row_42', 'row_43', 'row_44', 'row_45', 'row_46', 'row_47', 'row_48', 'row_49', 'row_50', 'row_51', 'row_52', 'row_53', 'row_54', 'row_55', 'row_56', 'row_57', 'row_58', 'row_59', 'row_60', 'row_61', 'row_62', 'row_63', 'row_64', 'row_65', 'row_66', 'row_67', 'row_68', 'row_69', 'row_70', 'row_71', 'row_72', 'row_73', 'row_74', 'row_75', 'row_76', 'row_77', 'row_78', 'row_79', 'row_80', 'row_81', 'row_82', 'row_83', 'row_84', 'row_85', 'row_86', 'row_87', 'row_88', 'row_89', 'row_90', 'row_91', 'row_92', 'row_93', 'row_94', 'row_95', 'row_96', 'row_97', 'row_98', 'row_99', 'row_100', 'row_101', 'row_102', 'row_103', 'row_104', 'row_105', 'row_106', 'row_107', 'row_108', 'row_109', 'row_110', 'row_111', 'row_112', 'row_113', 'row_114', 'row_115', 'row_116', 'row_117', 'row_118', 'row_119', 'row_120', 'row_121', 'row_122', 'row_123', 'row_124', 'row_125', 'row_126', 'row_127', 'row_128', 'row_129', 'row_130', 'row_131', 'row_132', 'row_133', 'row_134', 'row_135', 'row_136', 'row_137', 'row_138', 'row_139', 'row_140', 'row_141', 'row_142', 'row_143', 'row_144', 'row_145', 'row_146', 'row_147', 'row_148', 'row_149', 'row_150', 'row_151', 'row_152', 'row_153', 'row_154', 'row_155', 'row_156', 'row_157', 'row_158', 'row_159', 'row_160', 'row_161', 'row_162', 'row_163', 'row_164', 'row_165', 'row_166', 'row_167', 'row_168', 'row_169', 'row_170', 'row_171', 'row_172', 'row_173', 'row_174', 'row_175', 'row_176', 'row_177', 'row_178', 'row_179', 'row_180', 'row_181', 'row_182', 'row_183', 'row_184', 'row_185', 'row_186', 'row_187', 'row_188', 'row_189', 'row_190', 'row_191', 'row_192', 'row_193', 'row_194', 'row_195', 'row_196', 'row_197', 'row_198', 'row_199', 'row_200', 'row_201', 'row_202', 'row_203', 'row_204', 'row_205', 'row_206', 'row_207', 'row_208', 'row_209', 'row_210', 'row_211', 'row_212', 'row_213', 'row_214', 'row_215', 'row_216', 'row_217', 'row_218', 'row_219', 'row_220', 'row_221', 'row_222', 'row_223', 'row_224', 'row_225', 'row_226', 'row_227', 'row_228', 'row_229', 'row_230', 'row_231', 'row_232', 'row_233', 'row_234', 'row_235', 'row_236', 'row_237', 'row_238', 'row_239', 'row_240', 'row_241', 'row_242', 'row_243', 'row_244', 'row_245', 'row_246', 'row_247', 'row_248', 'row_249', 'row_250', 'row_251', 'row_252', 'row_253', 'row_254', 'row_255', 'row_256', 'row_257', 'row_258', 'row_259', 'row_260', 'row_261', 'row_262', 'row_263', 'row_264', 'row_265', 'row_266', 'row_267', 'row_268', 'row_269', 'row_270', 'row_271', 'row_272', 'row_273', 'row_274', 'row_275', 'row_276', 'row_277', 'row_278', 'row_279', 'row_280', 'row_281', 'row_282', 'row_283', 'row_284', 'row_285', 'row_286', 'row_287', 'row_288', 'row_289', 'row_290', 'row_291', 'row_292', 'row_293', 'row_294', 'row_295', 'row_296', 'row_297', 'row_298', 'row_299', 'row_300', 'row_301', 'row_302', 'row_303', 'row_304', 'row_305', 'row_306', 'row_307', 'row_308', 'row_309', 'row_310', 'row_311', 'row_312', 'row_313', 'row_314', 'row_315', 'row_316', 'row_317', 'row_318', 'row_319', 'row_320', 'row_321', 'row_322', 'row_323', 'row_324', 'row_325', 'row_326', 'row_327', 'row_328', 'row_329', 'row_330', 'row_331', 'row_332', 'row_333', 'row_334', 'row_335', 'row_336', 'row_337', 'row_338', 'row_339', 'row_340', 'row_341', 'row_342', 'row_343', 'row_344', 'row_345', 'row_346', 'row_347', 'row_348', 'row_349', 'row_350', 'row_351', 'row_352', 'row_353', 'row_354', 'row_355', 'row_356', 'row_357', 'row_358', 'row_359', 'row_360', 'row_361', 'row_362', 'row_363', 'row_364', 'row_365', 'row_366', 'row_367', 'row_368', 'row_369', 'row_370', 'row_371', 'row_372', 'row_373', 'row_374', 'row_375', 'row_376', 'row_377', 'row_378', 'row_379', 'row_380', 'row_381', 'row_382', 'row_383', 'row_384', 'row_385', 'row_386', 'row_387', 'row_388', 'row_389', 'row_390', 'row_391', 'row_392', 'row_393', 'row_394', 'row_395', 'row_396', 'row_397', 'row_398', 'row_399', 'row_400', 'row_401', 'row_402', 'row_403', 'row_404', 'row_405', 'row_406', 'row_407', 'row_408', 'row_409', 'row_410', 'row_411', 'row_412', 'row_413', 'row_414', 'row_415', 'row_416', 'row_417', 'row_418', 'row_419', 'row_420', 'row_421', 'row_422', 'row_423', 'row_424', 'row_425', 'row_426', 'row_427', 'row_428', 'row_429', 'row_430', 'row_431', 'row_432', 'row_433', 'row_434', 'row_435', 'row_436', 'row_437', 'row_438', 'row_439', 'row_440', 'row_441', 'row_442', 'row_443', 'row_444', 'row_445', 'row_446', 'row_447', 'row_448', 'row_449', 'row_450', 'row_451', 'row_452', 'row_453', 'row_454', 'row_455', 'row_456', 'row_457', 'row_458', 'row_459', 'row_460', 'row_461'};
predictors = inputTable(:, predictorNames);
response = inputTable.row_462;
isCategoricalPredictor = [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false];

% Train a classifier
% This code specifies all the classifier options and trains the classifier.
% For logistic regression, the response values must be converted to zeros
% and ones because the responses are assumed to follow a binomial
% distribution.
% 1 or true = 'successful' class
% 0 or false = 'failure' class
% NaN - missing response.
successClass = double(2);
failureClass = double(1);
% Compute the majority response class. If there is a NaN-prediction from
% fitglm, convert NaN to this majority class label.
numSuccess = sum(response == successClass);
numFailure = sum(response == failureClass);
if numSuccess > numFailure
    missingClass = successClass;
else
    missingClass = failureClass;
end
successFailureAndMissingClasses = [successClass; failureClass; missingClass];
isMissing = isnan(response);
zeroOneResponse = double(ismember(response, successClass));
zeroOneResponse(isMissing) = NaN;
% Prepare input arguments to fitglm.
concatenatedPredictorsAndResponse = [predictors, table(zeroOneResponse)];
% Train using fitglm.
GeneralizedLinearModel = fitglm(...
    concatenatedPredictorsAndResponse, ...
    'Distribution', 'binomial', ...
    'link', 'logit');

% Convert predicted probabilities to predicted class labels and scores.
convertSuccessProbsToPredictions = @(p) successFailureAndMissingClasses( ~isnan(p).*( (p<0.5) + 1 ) + isnan(p)*3 );
returnMultipleValuesFcn = @(varargin) varargin{1:max(1,nargout)};
scoresFcn = @(p) [1-p, p];
predictionsAndScoresFcn = @(p) returnMultipleValuesFcn( convertSuccessProbsToPredictions(p), scoresFcn(p) );

% Create the result struct with predict function
predictorExtractionFcn = @(x) array2table(x', 'VariableNames', predictorNames);
logisticRegressionPredictFcn = @(x) predictionsAndScoresFcn( predict(GeneralizedLinearModel, x) );
trainedClassifier.predictFcn = @(x) logisticRegressionPredictFcn(predictorExtractionFcn(x));

% Add additional fields to the result struct
trainedClassifier.GeneralizedLinearModel = GeneralizedLinearModel;
trainedClassifier.SuccessClass = successClass;
trainedClassifier.FailureClass = failureClass;
trainedClassifier.MissingClass = missingClass;
trainedClassifier.ClassNames = {successClass; failureClass};
trainedClassifier.About = 'This struct is a trained model exported from Classification Learner R2017b.';
trainedClassifier.HowToPredict = sprintf('To make predictions on a new predictor row matrix, X, use: \n  yfit = c.predictFcn(X) \nreplacing ''c'' with the name of the variable that is this struct, e.g. ''trainedModel''. \n \nX must contain exactly 461 rows because this model was trained using 461 predictors. \nX must contain only predictor rows in exactly the same order and format as your training \ndata. Do not include the response row or any rows you did not import into the app. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');

% Extract predictors and response
% This code processes the data into the right shape for training the
% model.
% Convert input to table
inputTable = array2table(trainingData', 'VariableNames', {'row_1', 'row_2', 'row_3', 'row_4', 'row_5', 'row_6', 'row_7', 'row_8', 'row_9', 'row_10', 'row_11', 'row_12', 'row_13', 'row_14', 'row_15', 'row_16', 'row_17', 'row_18', 'row_19', 'row_20', 'row_21', 'row_22', 'row_23', 'row_24', 'row_25', 'row_26', 'row_27', 'row_28', 'row_29', 'row_30', 'row_31', 'row_32', 'row_33', 'row_34', 'row_35', 'row_36', 'row_37', 'row_38', 'row_39', 'row_40', 'row_41', 'row_42', 'row_43', 'row_44', 'row_45', 'row_46', 'row_47', 'row_48', 'row_49', 'row_50', 'row_51', 'row_52', 'row_53', 'row_54', 'row_55', 'row_56', 'row_57', 'row_58', 'row_59', 'row_60', 'row_61', 'row_62', 'row_63', 'row_64', 'row_65', 'row_66', 'row_67', 'row_68', 'row_69', 'row_70', 'row_71', 'row_72', 'row_73', 'row_74', 'row_75', 'row_76', 'row_77', 'row_78', 'row_79', 'row_80', 'row_81', 'row_82', 'row_83', 'row_84', 'row_85', 'row_86', 'row_87', 'row_88', 'row_89', 'row_90', 'row_91', 'row_92', 'row_93', 'row_94', 'row_95', 'row_96', 'row_97', 'row_98', 'row_99', 'row_100', 'row_101', 'row_102', 'row_103', 'row_104', 'row_105', 'row_106', 'row_107', 'row_108', 'row_109', 'row_110', 'row_111', 'row_112', 'row_113', 'row_114', 'row_115', 'row_116', 'row_117', 'row_118', 'row_119', 'row_120', 'row_121', 'row_122', 'row_123', 'row_124', 'row_125', 'row_126', 'row_127', 'row_128', 'row_129', 'row_130', 'row_131', 'row_132', 'row_133', 'row_134', 'row_135', 'row_136', 'row_137', 'row_138', 'row_139', 'row_140', 'row_141', 'row_142', 'row_143', 'row_144', 'row_145', 'row_146', 'row_147', 'row_148', 'row_149', 'row_150', 'row_151', 'row_152', 'row_153', 'row_154', 'row_155', 'row_156', 'row_157', 'row_158', 'row_159', 'row_160', 'row_161', 'row_162', 'row_163', 'row_164', 'row_165', 'row_166', 'row_167', 'row_168', 'row_169', 'row_170', 'row_171', 'row_172', 'row_173', 'row_174', 'row_175', 'row_176', 'row_177', 'row_178', 'row_179', 'row_180', 'row_181', 'row_182', 'row_183', 'row_184', 'row_185', 'row_186', 'row_187', 'row_188', 'row_189', 'row_190', 'row_191', 'row_192', 'row_193', 'row_194', 'row_195', 'row_196', 'row_197', 'row_198', 'row_199', 'row_200', 'row_201', 'row_202', 'row_203', 'row_204', 'row_205', 'row_206', 'row_207', 'row_208', 'row_209', 'row_210', 'row_211', 'row_212', 'row_213', 'row_214', 'row_215', 'row_216', 'row_217', 'row_218', 'row_219', 'row_220', 'row_221', 'row_222', 'row_223', 'row_224', 'row_225', 'row_226', 'row_227', 'row_228', 'row_229', 'row_230', 'row_231', 'row_232', 'row_233', 'row_234', 'row_235', 'row_236', 'row_237', 'row_238', 'row_239', 'row_240', 'row_241', 'row_242', 'row_243', 'row_244', 'row_245', 'row_246', 'row_247', 'row_248', 'row_249', 'row_250', 'row_251', 'row_252', 'row_253', 'row_254', 'row_255', 'row_256', 'row_257', 'row_258', 'row_259', 'row_260', 'row_261', 'row_262', 'row_263', 'row_264', 'row_265', 'row_266', 'row_267', 'row_268', 'row_269', 'row_270', 'row_271', 'row_272', 'row_273', 'row_274', 'row_275', 'row_276', 'row_277', 'row_278', 'row_279', 'row_280', 'row_281', 'row_282', 'row_283', 'row_284', 'row_285', 'row_286', 'row_287', 'row_288', 'row_289', 'row_290', 'row_291', 'row_292', 'row_293', 'row_294', 'row_295', 'row_296', 'row_297', 'row_298', 'row_299', 'row_300', 'row_301', 'row_302', 'row_303', 'row_304', 'row_305', 'row_306', 'row_307', 'row_308', 'row_309', 'row_310', 'row_311', 'row_312', 'row_313', 'row_314', 'row_315', 'row_316', 'row_317', 'row_318', 'row_319', 'row_320', 'row_321', 'row_322', 'row_323', 'row_324', 'row_325', 'row_326', 'row_327', 'row_328', 'row_329', 'row_330', 'row_331', 'row_332', 'row_333', 'row_334', 'row_335', 'row_336', 'row_337', 'row_338', 'row_339', 'row_340', 'row_341', 'row_342', 'row_343', 'row_344', 'row_345', 'row_346', 'row_347', 'row_348', 'row_349', 'row_350', 'row_351', 'row_352', 'row_353', 'row_354', 'row_355', 'row_356', 'row_357', 'row_358', 'row_359', 'row_360', 'row_361', 'row_362', 'row_363', 'row_364', 'row_365', 'row_366', 'row_367', 'row_368', 'row_369', 'row_370', 'row_371', 'row_372', 'row_373', 'row_374', 'row_375', 'row_376', 'row_377', 'row_378', 'row_379', 'row_380', 'row_381', 'row_382', 'row_383', 'row_384', 'row_385', 'row_386', 'row_387', 'row_388', 'row_389', 'row_390', 'row_391', 'row_392', 'row_393', 'row_394', 'row_395', 'row_396', 'row_397', 'row_398', 'row_399', 'row_400', 'row_401', 'row_402', 'row_403', 'row_404', 'row_405', 'row_406', 'row_407', 'row_408', 'row_409', 'row_410', 'row_411', 'row_412', 'row_413', 'row_414', 'row_415', 'row_416', 'row_417', 'row_418', 'row_419', 'row_420', 'row_421', 'row_422', 'row_423', 'row_424', 'row_425', 'row_426', 'row_427', 'row_428', 'row_429', 'row_430', 'row_431', 'row_432', 'row_433', 'row_434', 'row_435', 'row_436', 'row_437', 'row_438', 'row_439', 'row_440', 'row_441', 'row_442', 'row_443', 'row_444', 'row_445', 'row_446', 'row_447', 'row_448', 'row_449', 'row_450', 'row_451', 'row_452', 'row_453', 'row_454', 'row_455', 'row_456', 'row_457', 'row_458', 'row_459', 'row_460', 'row_461', 'row_462'});

predictorNames = {'row_1', 'row_2', 'row_3', 'row_4', 'row_5', 'row_6', 'row_7', 'row_8', 'row_9', 'row_10', 'row_11', 'row_12', 'row_13', 'row_14', 'row_15', 'row_16', 'row_17', 'row_18', 'row_19', 'row_20', 'row_21', 'row_22', 'row_23', 'row_24', 'row_25', 'row_26', 'row_27', 'row_28', 'row_29', 'row_30', 'row_31', 'row_32', 'row_33', 'row_34', 'row_35', 'row_36', 'row_37', 'row_38', 'row_39', 'row_40', 'row_41', 'row_42', 'row_43', 'row_44', 'row_45', 'row_46', 'row_47', 'row_48', 'row_49', 'row_50', 'row_51', 'row_52', 'row_53', 'row_54', 'row_55', 'row_56', 'row_57', 'row_58', 'row_59', 'row_60', 'row_61', 'row_62', 'row_63', 'row_64', 'row_65', 'row_66', 'row_67', 'row_68', 'row_69', 'row_70', 'row_71', 'row_72', 'row_73', 'row_74', 'row_75', 'row_76', 'row_77', 'row_78', 'row_79', 'row_80', 'row_81', 'row_82', 'row_83', 'row_84', 'row_85', 'row_86', 'row_87', 'row_88', 'row_89', 'row_90', 'row_91', 'row_92', 'row_93', 'row_94', 'row_95', 'row_96', 'row_97', 'row_98', 'row_99', 'row_100', 'row_101', 'row_102', 'row_103', 'row_104', 'row_105', 'row_106', 'row_107', 'row_108', 'row_109', 'row_110', 'row_111', 'row_112', 'row_113', 'row_114', 'row_115', 'row_116', 'row_117', 'row_118', 'row_119', 'row_120', 'row_121', 'row_122', 'row_123', 'row_124', 'row_125', 'row_126', 'row_127', 'row_128', 'row_129', 'row_130', 'row_131', 'row_132', 'row_133', 'row_134', 'row_135', 'row_136', 'row_137', 'row_138', 'row_139', 'row_140', 'row_141', 'row_142', 'row_143', 'row_144', 'row_145', 'row_146', 'row_147', 'row_148', 'row_149', 'row_150', 'row_151', 'row_152', 'row_153', 'row_154', 'row_155', 'row_156', 'row_157', 'row_158', 'row_159', 'row_160', 'row_161', 'row_162', 'row_163', 'row_164', 'row_165', 'row_166', 'row_167', 'row_168', 'row_169', 'row_170', 'row_171', 'row_172', 'row_173', 'row_174', 'row_175', 'row_176', 'row_177', 'row_178', 'row_179', 'row_180', 'row_181', 'row_182', 'row_183', 'row_184', 'row_185', 'row_186', 'row_187', 'row_188', 'row_189', 'row_190', 'row_191', 'row_192', 'row_193', 'row_194', 'row_195', 'row_196', 'row_197', 'row_198', 'row_199', 'row_200', 'row_201', 'row_202', 'row_203', 'row_204', 'row_205', 'row_206', 'row_207', 'row_208', 'row_209', 'row_210', 'row_211', 'row_212', 'row_213', 'row_214', 'row_215', 'row_216', 'row_217', 'row_218', 'row_219', 'row_220', 'row_221', 'row_222', 'row_223', 'row_224', 'row_225', 'row_226', 'row_227', 'row_228', 'row_229', 'row_230', 'row_231', 'row_232', 'row_233', 'row_234', 'row_235', 'row_236', 'row_237', 'row_238', 'row_239', 'row_240', 'row_241', 'row_242', 'row_243', 'row_244', 'row_245', 'row_246', 'row_247', 'row_248', 'row_249', 'row_250', 'row_251', 'row_252', 'row_253', 'row_254', 'row_255', 'row_256', 'row_257', 'row_258', 'row_259', 'row_260', 'row_261', 'row_262', 'row_263', 'row_264', 'row_265', 'row_266', 'row_267', 'row_268', 'row_269', 'row_270', 'row_271', 'row_272', 'row_273', 'row_274', 'row_275', 'row_276', 'row_277', 'row_278', 'row_279', 'row_280', 'row_281', 'row_282', 'row_283', 'row_284', 'row_285', 'row_286', 'row_287', 'row_288', 'row_289', 'row_290', 'row_291', 'row_292', 'row_293', 'row_294', 'row_295', 'row_296', 'row_297', 'row_298', 'row_299', 'row_300', 'row_301', 'row_302', 'row_303', 'row_304', 'row_305', 'row_306', 'row_307', 'row_308', 'row_309', 'row_310', 'row_311', 'row_312', 'row_313', 'row_314', 'row_315', 'row_316', 'row_317', 'row_318', 'row_319', 'row_320', 'row_321', 'row_322', 'row_323', 'row_324', 'row_325', 'row_326', 'row_327', 'row_328', 'row_329', 'row_330', 'row_331', 'row_332', 'row_333', 'row_334', 'row_335', 'row_336', 'row_337', 'row_338', 'row_339', 'row_340', 'row_341', 'row_342', 'row_343', 'row_344', 'row_345', 'row_346', 'row_347', 'row_348', 'row_349', 'row_350', 'row_351', 'row_352', 'row_353', 'row_354', 'row_355', 'row_356', 'row_357', 'row_358', 'row_359', 'row_360', 'row_361', 'row_362', 'row_363', 'row_364', 'row_365', 'row_366', 'row_367', 'row_368', 'row_369', 'row_370', 'row_371', 'row_372', 'row_373', 'row_374', 'row_375', 'row_376', 'row_377', 'row_378', 'row_379', 'row_380', 'row_381', 'row_382', 'row_383', 'row_384', 'row_385', 'row_386', 'row_387', 'row_388', 'row_389', 'row_390', 'row_391', 'row_392', 'row_393', 'row_394', 'row_395', 'row_396', 'row_397', 'row_398', 'row_399', 'row_400', 'row_401', 'row_402', 'row_403', 'row_404', 'row_405', 'row_406', 'row_407', 'row_408', 'row_409', 'row_410', 'row_411', 'row_412', 'row_413', 'row_414', 'row_415', 'row_416', 'row_417', 'row_418', 'row_419', 'row_420', 'row_421', 'row_422', 'row_423', 'row_424', 'row_425', 'row_426', 'row_427', 'row_428', 'row_429', 'row_430', 'row_431', 'row_432', 'row_433', 'row_434', 'row_435', 'row_436', 'row_437', 'row_438', 'row_439', 'row_440', 'row_441', 'row_442', 'row_443', 'row_444', 'row_445', 'row_446', 'row_447', 'row_448', 'row_449', 'row_450', 'row_451', 'row_452', 'row_453', 'row_454', 'row_455', 'row_456', 'row_457', 'row_458', 'row_459', 'row_460', 'row_461'};
predictors = inputTable(:, predictorNames);
response = inputTable.row_462;
isCategoricalPredictor = [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false];

% Perform cross-validation
KFolds = 5;
cvp = cvpartition(response, 'KFold', KFolds);
% Initialize the predictions to the proper sizes
validationPredictions = response;
numObservations = size(predictors, 1);
numClasses = 2;
validationScores = NaN(numObservations, numClasses);
for fold = 1:KFolds
    trainingPredictors = predictors(cvp.training(fold), :);
    trainingResponse = response(cvp.training(fold), :);
    foldIsCategoricalPredictor = isCategoricalPredictor;
    
    % Train a classifier
    % This code specifies all the classifier options and trains the classifier.
    % For logistic regression, the response values must be converted to zeros
    % and ones because the responses are assumed to follow a binomial
    % distribution.
    % 1 or true = 'successful' class
    % 0 or false = 'failure' class
    % NaN - missing response.
    successClass = double(2);
    failureClass = double(1);
    % Compute the majority response class. If there is a NaN-prediction from
    % fitglm, convert NaN to this majority class label.
    numSuccess = sum(trainingResponse == successClass);
    numFailure = sum(trainingResponse == failureClass);
    if numSuccess > numFailure
        missingClass = successClass;
    else
        missingClass = failureClass;
    end
    successFailureAndMissingClasses = [successClass; failureClass; missingClass];
    isMissing = isnan(trainingResponse);
    zeroOneResponse = double(ismember(trainingResponse, successClass));
    zeroOneResponse(isMissing) = NaN;
    % Prepare input arguments to fitglm.
    concatenatedPredictorsAndResponse = [trainingPredictors, table(zeroOneResponse)];
    % Train using fitglm.
    GeneralizedLinearModel = fitglm(...
        concatenatedPredictorsAndResponse, ...
        'Distribution', 'binomial', ...
        'link', 'logit');
    
    % Convert predicted probabilities to predicted class labels and scores.
    convertSuccessProbsToPredictions = @(p) successFailureAndMissingClasses( ~isnan(p).*( (p<0.5) + 1 ) + isnan(p)*3 );
    returnMultipleValuesFcn = @(varargin) varargin{1:max(1,nargout)};
    scoresFcn = @(p) [1-p, p];
    predictionsAndScoresFcn = @(p) returnMultipleValuesFcn( convertSuccessProbsToPredictions(p), scoresFcn(p) );
    
    % Create the result struct with predict function
    logisticRegressionPredictFcn = @(x) predictionsAndScoresFcn( predict(GeneralizedLinearModel, x) );
    validationPredictFcn = @(x) logisticRegressionPredictFcn(x);
    
    % Add additional fields to the result struct
    
    % Compute validation predictions
    validationPredictors = predictors(cvp.test(fold), :);
    [foldPredictions, foldScores] = validationPredictFcn(validationPredictors);
    
    % Store predictions in the original order
    validationPredictions(cvp.test(fold), :) = foldPredictions;
    validationScores(cvp.test(fold), :) = foldScores;
end

% Compute validation accuracy
correctPredictions = (validationPredictions == response);
isMissing = isnan(response);
correctPredictions = correctPredictions(~isMissing);
validationAccuracy = sum(correctPredictions)/length(correctPredictions);
